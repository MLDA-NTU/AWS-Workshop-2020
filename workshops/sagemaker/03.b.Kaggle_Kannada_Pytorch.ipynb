{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Series 04. Part II - Kaggle Workflow using Pytorch: Kannada MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the series. This is the second part of tutorial series #04. The overview of tutorial series #04 is as follows:\n",
    "\n",
    "- **Part I  :** Convolutional Neural Network  [[notebook](#04.a.Convolutional_Neural_Net)]\n",
    "- **Part II :** Hands-on with Kannada MNIST dataset using Pytorch (this notebook)\n",
    "- **Part III:** Pytorch model training and deployment in AWS Sagemaker [[notebook](#04.b.Kaggle_Kannada_Pytorch)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "\n",
    "1. [__Getting Started__](#100)\n",
    "    1. [__Background__](#110)\n",
    "    2. [__Goal__](#120)\n",
    "    3. [__Packages__](#130)\n",
    "2. [__Meet and Greet Data__](#200)\n",
    "    1. [__Downloading Kaggle Dataset__](#210)\n",
    "    2. [__Loading Dataset__](#220)\n",
    "    3. [__Visualising Dataset Distribution__](#230)\n",
    "    4. [__Cross Validation__](#240)\n",
    "3. [__Pytorch Dataset Object & Data Loader__](#300)\n",
    "    1. [__Pytorch Dataset Object__](#310)\n",
    "    2. [__Image Transformation Pipeline__](#320)\n",
    "    3. [__Pytorch Data loader__](#330)\n",
    "    4. [__Visualising Kannada Image__](#340)\n",
    "4. [__Building CNN Model__](#400)\n",
    "    1. [__Designing CNN using Pytorch__](#410)\n",
    "    2. [__Constructing Neural Network Object__](#420)\n",
    "5. [__Training Session__](#500)\n",
    "    1. [__Evaluation Metric__](#510)\n",
    "    2. [__Model Training__](#520)\n",
    "    3. [__Model Evaluation__](#530)\n",
    "    4. [__Run Complete Training Session__](#540)\n",
    "    5. [__Visualising Training Result__](#550)\n",
    "    6. [ __Writting Submission to Kaggle__](#560) [Optional]\n",
    "\n",
    "6. [__REFERENCES__](#600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Getting Started\n",
    "\n",
    "\n",
    "### 1.1 Background\n",
    "\n",
    "Kannada is a language spoken predominantly by people of Karnataka in southwestern India. The language has roughly 45 million native speakers and is written using the Kannada script. [Wikipedia](https://en.wikipedia.org/wiki/Kannada)\n",
    "\n",
    "\n",
    "![Kannada](images/kannada_banner.png)\n",
    "\n",
    "The challenge faced by many industry and banking instances in India is to build a Optical Character Recognition that can detect Kannada digits from handwriting in cheques, written documents, etc to automate important task. By building a simple digit recognition, we can help many bussiness which application relies on reading handwritten digits, e.g. automating thousands of transaction from bank cheque validation.\n",
    "\n",
    "A study in the paper of: Prabhu, Vinay Uday nicely captured the Kannada handwritten digits and the author has kindly shared with us the dataset we can experiment with. \"Kannada-MNIST: A new handwritten digits dataset for the Kannada language.\" arXiv preprint [arXiv:1908.01242 (2019)](https://arxiv.org/abs/1908.01242)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Goal\n",
    "This tutorial will show you how to build a convolutional neural network using Pytorch. The dataset we are using is the [Kannada MNIST dataset](https://www.kaggle.com/c/Kannada-MNIST), a playground dataset from Kaggle. Here you can expect to get an insight about:\n",
    "- Using Pytorch for data science modelling\n",
    "- Get to know the workflow when participating in data science competition in [Kaggle](https://www.kaggle.com/) platform\n",
    "- Understand about [Convolutional Neural Network](http://cs231n.stanford.edu/) for modelling Image Classification problem.\n",
    "\n",
    "For more information about the PyTorch, please visit [Pytorch official website](https://pytorch.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Packages\n",
    "\n",
    "We will also setup our project by specifying libraries and modules that we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /home/ec2-user/.local/lib/python3.6/site-packages (1.5.6)\n",
      "Requirement already satisfied: python-dateutil in /usr/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/.local/lib/python3.6/site-packages (from kaggle) (4.41.1)\n",
      "Requirement already satisfied: requests in /usr/lib/python3.6/dist-packages (from kaggle) (2.20.0)\n",
      "Requirement already satisfied: six>=1.10 in /usr/lib/python3.6/dist-packages (from kaggle) (1.13.0)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3.6/dist-packages (from kaggle) (2019.9.11)\n",
      "Requirement already satisfied: python-slugify in /home/ec2-user/.local/lib/python3.6/site-packages (from kaggle) (4.0.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /usr/lib/python3.6/dist-packages (from requests->kaggle) (2.7)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /home/ec2-user/.local/lib/python3.6/site-packages (from python-slugify->kaggle) (1.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 19.0.2, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip3 install --user kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library\n",
    "import os, sys\n",
    "import math\n",
    "import json, logging, argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data analytics library\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch Library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Pytorch Utilities and Vision Library\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "try:\n",
    "    from torchsummary import summary\n",
    "except:\n",
    "    summary = print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table {\n",
       "        display: inline-block\n",
       "    }\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "    table {\n",
    "        display: inline-block\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_17929172072181135079() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_17929172072181135079()\">Toggle show/hide</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "import random\n",
    "\n",
    "\n",
    "def hide_toggle(for_next=False):\n",
    "    this_cell = \"\"\"$('div.cell.code_cell.rendered.selected')\"\"\"\n",
    "    next_cell = this_cell + '.next()'\n",
    "\n",
    "    toggle_text = 'Toggle show/hide'  # text shown on toggle link\n",
    "    target_cell = this_cell  # target cell to control with toggle\n",
    "    js_hide_current = ''  # bit of JS to permanently hide code in current cell (only when toggling next cell)\n",
    "\n",
    "    if for_next:\n",
    "        target_cell = next_cell\n",
    "        toggle_text += ' next cell'\n",
    "        js_hide_current = this_cell + '.find(\"div.input\").hide();'\n",
    "\n",
    "    js_f_name = 'code_toggle_{}'.format(str(random.randint(1,2**64)))\n",
    "\n",
    "    html = \"\"\"\n",
    "        <script>\n",
    "            function {f_name}() {{\n",
    "                {cell_selector}.find('div.input').toggle();\n",
    "            }}\n",
    "\n",
    "            {js_hide_current}\n",
    "        </script>\n",
    "\n",
    "        <a href=\"javascript:{f_name}()\">{toggle_text}</a>\n",
    "    \"\"\".format(\n",
    "        f_name=js_f_name,\n",
    "        cell_selector=target_cell,\n",
    "        js_hide_current=js_hide_current, \n",
    "        toggle_text=toggle_text\n",
    "    )\n",
    "    return HTML(html)\n",
    "\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meet and Greet Data, Kaggle Kannada MNIST\n",
    "_(Duration: 20 min)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/Kannada'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Kaggle Dataset\n",
    "\n",
    "In this workshop, we will quickly demonstrate how to download kaggle datasets into our jupyter environment. Kaggle has been and remains the de factor platform to try your hands on data science projects. The platform has huge rich free datasets for machine learning projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling kaggle-1.5.6:\n",
      "  Successfully uninstalled kaggle-1.5.6\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip uninstall -y kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "## Copy API key file to where Kaggle expects it\n",
    "## Make sure to download the kaggle key file next to this notebook\n",
    "mkdir -p ~/.kaggle\n",
    "cp kaggle.json ~/.kaggle/kaggle.json && chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we downloaded the data using Kaggle client and make an API call to stream the data. Once we receive the data, we unzip them into a folder we have prepared in `data/Kannada/raw`\n",
    "\n",
    "Using the credential, kaggle API will be able to authenticate us. Then the download begin when we run\n",
    "\n",
    "```\n",
    "$ kaggle competitions download -c Kannada-MNIST```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: line 2: kaggle: command not found\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## Download Kannada MNIST from Kaggle\n",
    "kaggle competitions download -c Kannada-MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we unzip the data once we finish the download to a folder called `data/Kannada/raw`. We will also create a directory in `data/Kannada/processed` to save our processed dataset later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "## Create our data directory\n",
    "mkdir -p data/Kannada/raw\n",
    "mkdir -p data/Kannada/processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "## Unzip to data/Kannada directory\n",
    "unzip Kannada-MNIST.zip -d data/Kannada/raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always a good practice to seperate raw data we just downloaded into a folder called 'raw'. Along the process, we will encounter a situation where we have pre-processed/cleaned our dataset and would like to store them seperately from the raw data.\n",
    "\n",
    "Now let's check the content of our dataset in `raw` folder. We see several files here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data folder content\n",
    "for dirname, _, filenames in os.walk(data_dir):\n",
    "    for filename in filenames:\n",
    "        print('data at: ' + os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Files\n",
    "\n",
    "There are 4 files we get after downloading the data from Kaggle. According to the official docs above, the files summary is as follows:\n",
    "- `train.csv` - the training set. The first column of every row is the label, the rest 784 values in the same row is the pixel value of our image with flattened structure defined above.\n",
    "- `Dig-MNIST.csv`: this is similar to image training set in `train.csv`. The contributor provide this set kindly to allow us to have a validation/testing set before we make a submission.\n",
    "- `test.csv`: the submission set. Unlike the `train.csv` and `Dig-MNIST.csv` it doesn't come with label because it suppose to be our submission for the competition. We will refer data from this file as *submission set* from now on.\n",
    "- `sample_submission.csv` - a sample submission file in the correct format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Load Data\n",
    "train = pd.read_csv(os.path.join(data_dir, 'raw/train.csv'))\n",
    "test = pd.read_csv(os.path.join(data_dir, 'raw/Dig-MNIST.csv'))  # this will be our test set for evaluation\n",
    "submission_set = pd.read_csv(os.path.join(data_dir, 'raw/test.csv')).iloc[:,1:]  # this set is only for submission\n",
    "\n",
    "# # Seperate train data and labels\n",
    "train_data = train.drop('label',axis=1)\n",
    "train_targets = train['label']\n",
    "\n",
    "# # Seperate test data and labels\n",
    "test_images=test.drop('label',axis=1)\n",
    "test_labels=test['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We have $28$x$28$ dimension handwritten pics.\n",
    "* Dataset has been already flattened and has 784-pixel values for each pic.\n",
    "* Totaly we have $60000$ pics in training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "\n",
    "Let's look at the content of the files. There are things to take note regarding our data:\n",
    "- The content of our file is flattened image. Originally, each row was a 28 x 28 pixels image. \n",
    "- The each image is flattened into 784 row and saved into a `.csv` file. \n",
    "- The image itself is `Gray` image, which means it only has one channel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(10).iloc[:,490:520]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising Dataset Distribution\n",
    "\n",
    "* It is important to know the distribution of data according to the labels they have.\n",
    "* This data set is __homogeneously__ distributed as you see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dist = train_labels.value_counts(normalize = True)\n",
    "test_dist = test_labels.value_counts(normalize = True)\n",
    "submission_dist = train_labels.value_counts(normalize = True)\n",
    "\n",
    "dists_to_plot = [('Trainset', train_dist), ('Testset', submission_dist)]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n",
    "colors = []\n",
    "\n",
    "for i, (name, df) in enumerate(dists_to_plot):\n",
    "    sns.barplot(x=df.index, y=df, ax=axes[i])\n",
    "    axes[i].set_title(name + ' distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation: Training - Validation - Test Split \n",
    "\n",
    "**Splitting training set to train and validation set** - In order to measure the generalization ability of the model, we train the data with the training set and make the model arrangement according to the error value in the validation set. In addition, we determine the final performance of the model with the test set.\n",
    "\n",
    "This Kernel is prepared on a Kaggle competition dataset. They give us a training set for training the model and a test set without labels for prediction. As we don't have the labels we don't know the final performance of the test set until we submit our predictions.\n",
    "\n",
    "**Why we need testing set? Isn't validation set enough?** - -The reason for using the test set on the final evaluation is the model would have a bias on the validation set because we developed the model according to the validation set performance. So a kind of overfitting on the validation set is formed. The testing set will be performed to further evaluate the model after validation set. As mentioned above, this is to check whether our modelling has been overfitted/biased toward the validation set. \n",
    "\n",
    "To evaluate the model better, we need to split our training set into training and validation set. We will use a testing set from `Dig-MNIST.csv` instead which has been prepared for the purpose of evaluation. \n",
    "\n",
    "Commonly, the prefered split for training set:\n",
    "  * Training set -  $80$% \n",
    "  * Validation set -  $20$%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now how do we split our training and validation set? For simple splitting scheme, we can borrow the functionality from `sklearn.model_selection.train_test_split` as follows. How about testing set? Well we already have it in `test_images` dataframe. Hence we only need to split on training set to train and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split for evaluation\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_data, \n",
    "                                                                     train_targets, \n",
    "                                                                     test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Index\n",
    "train_images.reset_index(drop=True, inplace=True)\n",
    "train_labels.reset_index(drop=True, inplace=True)\n",
    "\n",
    "val_images.reset_index(drop=True, inplace=True)\n",
    "val_labels.reset_index(drop=True, inplace=True)\n",
    "\n",
    "test_images.reset_index(drop=True, inplace=True)\n",
    "test_labels.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting, our dataset is organised as follows:\n",
    "\n",
    "| Split      | Number of data | Shape of Image | Number of labels\n",
    "| ---:       | ---:           | ---:           |  ---:\n",
    "|Training    |  48000         |  1 x 28 x 28   |  48000\n",
    "|Validation  |  12000         |  1 x 28 x 28   |  12000\n",
    "|Testing     |  10240         |  1 x 28 x 28   |  10240\n",
    "|Submission  |  5000          |  1 x 28 x 28   |  5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of training set   :', train_images.shape)\n",
    "print('Shape of validation set :', val_images.shape)\n",
    "print('Shape of testing set    :', test_images.shape)\n",
    "\n",
    "print('Shape of submission set :', submission_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Dataset Split\n",
    "\n",
    "Now that we have done several splitting step, let's save our data that we have at this point. This is a good practice so that the next time you would like to continue your work, you don't have to run the code from the beggining. Writing the data could take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_dir = os.path.join(data_dir, 'processed/')\n",
    "\n",
    "# Save data to local folder first\n",
    "train_images.to_csv(processed_data_dir + 'train.csv', index=False, header=False)\n",
    "train_labels.to_csv(processed_data_dir + 'train_labels.csv', index=False, header=False)\n",
    "\n",
    "val_images.to_csv(processed_data_dir + 'validation.csv', index=False, header=False)\n",
    "val_labels.to_csv(processed_data_dir + 'validation_labels.csv', index=False, header=False)\n",
    "\n",
    "test_images.to_csv(processed_data_dir + 'test.csv', index=False, header=False)\n",
    "test_labels.to_csv(processed_data_dir + 'test_labels.csv', index=False, header=False)\n",
    "\n",
    "submission_set.to_csv(processed_data_dir + 'submission.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Dataset Object & Data Loader\n",
    "\n",
    "A lot of effort in solving any machine learning problem goes in to preparing the data. PyTorch provides many tools to make data loading easy and hopefully, to make your code more readable. Let's see how we can simply to load and preprocess/augment data from our Kannada dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Dataset Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our raw dataset originally came from a `.csv` format. This table is represented by `pd.DataFrame` object which support Microsoft-Excel-like functionality on table manipulation.\n",
    "\n",
    "However, our Kannada dataset is meant to be an image, not table dataset. If you look carefully, our data is a flattened image pixels, stored in a row of a table. The problem here is that representing image with `pd.DataFrame` table is unheard off as we will find difficulties in performing image operations, e.g. filtering, rotation, and convolution.\n",
    "\n",
    "Let's see how we can mitigate this problem, and automate them in Pytorch `Dataset object`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KannadaDataSet(Dataset):\n",
    "    \"\"\" Representation of our Kannada Dataset as a whole.\n",
    "    Scope of this object is to dictate how to access a single data/image and\n",
    "    it's label from our datapool, then convert it to desired matrix format which represent\n",
    "    an image.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, labels, transforms=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        --------------------\n",
    "            .. X (pd.DataFrame): table representation of flattened Kannada image pixels\n",
    "            .. labels (array-like): ground truth label of the digit. Ranging from digit 0 to digit 9\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "         \n",
    "    def __len__(self):\n",
    "        return (len(self.X))\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        data = self.X.iloc[i,:]\n",
    "        data = np.array(data).astype(np.uint8).reshape(IMGSIZE,IMGSIZE,1)\n",
    "        \n",
    "        if self.transforms:\n",
    "            data = self.transforms(data)\n",
    "            \n",
    "        if self.labels is not None:\n",
    "            # for train set, val set, and test set\n",
    "            return (data, self.labels[i])\n",
    "        else:\n",
    "            # for kaggle submission\n",
    "            # since submission set will not have labels\n",
    "            return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Transformation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of data science workflow, we will need to write some prepocessing code. One issue with our dataset right now is that it needs to be converted from `np.ndarray` matrix form to `torch.Tensor`. \n",
    "\n",
    "In addition to that, we would like to introduce several data augmentation, such as rotation, and translation to our image to increase the variety of our dataset. This is done so that our model sees more variety of data and more robust to the effect of image translation and rotation. \n",
    "\n",
    "Let's define three transform:\n",
    "- **`ToPILImage`**: PIL Image representation allows image based operation. It's good to convert our matrix to PIL Image first\n",
    "- **`RandomCrop`**: to crop from image randomly. This is data augmentation, which introduce translation\n",
    "- **`RandomAffine`**: to rotate image randomly from -5 to 5 degrees. This is data augmentation\n",
    "- **`ToTensor`**: to convert the numpy images to torch images (we need to swap axes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMGSIZE = 28\n",
    "\n",
    "# Transformations for the train\n",
    "train_trans = transforms.Compose(([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomCrop(IMGSIZE),\n",
    "    transforms.RandomAffine(degrees=5, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(), # automatically divide pixels by 255\n",
    "]))\n",
    "\n",
    "# Transformations for the validation & test sets\n",
    "val_trans = transforms.Compose(([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(), # automatically divide pixels by 255\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Data Loader\n",
    "\n",
    "Loading data may take a few moments, and you should see your progress as the data is loading. You may also choose to increase the `batch_size` if you want to load more data at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Initialise dataset object for each set\n",
    "train_data = KannadaDataSet(train_images, train_labels, train_trans)\n",
    "val_data   = KannadaDataSet(val_images, val_labels, val_trans)\n",
    "test_data  = KannadaDataSet(test_images, test_labels, val_trans)\n",
    "submission_data = KannadaDataSet(submission_set, None, val_trans)\n",
    "\n",
    "# Define Dataloader for each set\n",
    "train_loader = DataLoader(train_data,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_data, \n",
    "                        batch_size=batch_size, # batch_size=1000\n",
    "                        shuffle=False)\n",
    "\n",
    "test_loader = DataLoader(test_data,\n",
    "                         batch_size=batch_size, # batch_size=1000\n",
    "                         shuffle=False)\n",
    "\n",
    "# for kaggle submission\n",
    "submission_loader = DataLoader(submission_data,\n",
    "                               batch_size=batch_size,\n",
    "                               shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Image in Training Batch\n",
    "\n",
    "The first step in a classification task is to take a look at the data, make sure it is loaded in correctly, then make any initial observations about patterns in that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 6))\n",
    "for idx in np.arange(16):\n",
    "    ax = fig.add_subplot(2, 16/2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    ax.set_title('digit ' + str(labels[idx].item()), fontsize=16)  # .item() gets single value in scalar tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Image in Detail\n",
    "\n",
    "Now let's see an image from MNIST dataset in detail. Notice how our image pixels only ranges from $(0, 1)$. This means that no further normalisation is required in the preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.squeeze(images[1])\n",
    "\n",
    "fig = plt.figure(figsize = (12,12)) \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')\n",
    "width, height = img.shape\n",
    "thresh = img.max()/2.5\n",
    "\n",
    "for x in range(width):\n",
    "    for y in range(height):\n",
    "        val = round(img[x][y],2) if img[x][y] !=0 else 0\n",
    "        ax.annotate(str(val), xy=(y,x),\n",
    "                    horizontalalignment='center',\n",
    "                    verticalalignment='center',\n",
    "                    color='white' if img[x][y]<thresh else 'black')\n",
    "\n",
    "ax.set_title('Kannada Digit in detail: label %d' % labels[1].item());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Building CNN Model\n",
    "_(Duration: 25 min)_\n",
    "\n",
    "On building stage you specify the architecture of the model mainly.\n",
    "\n",
    "You can decide the Filter size and Padding type you will use on Convolution operations and add Pooling, Batch Normalization, Dropout, activation function layers with build section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 1 Designing CNN using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KannadaCNN(nn.Module):\n",
    "    \"\"\" Simple Convolutional Neural Network to classify \n",
    "    Kannada handwritten digit\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_p=0.4, num_classes=10):\n",
    "        \"\"\" Initialise and build network layers \n",
    "        Arguments\n",
    "        --------------------\n",
    "            .. drop_p (float, range[0, 1.]): constant probability in our dropout layer\n",
    "            .. num_classes (int): number of target classes in our data\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # First hidden layer\n",
    "        self.conv2d_0 = nn.Conv2d(1, 64, kernel_size=5, padding=2)\n",
    "        self.convbn_0 = nn.BatchNorm2d(num_features=64)\n",
    "\n",
    "        self.conv2d_1 = nn.Conv2d(64, 64, kernel_size=5, padding=2)\n",
    "        self.convbn_1 = nn.BatchNorm2d(num_features=64)\n",
    "\n",
    "        self.pool_1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.drop_1 = nn.Dropout2d(p=drop_p)\n",
    "\n",
    "        # Second hidden layer\n",
    "        self.conv2d_2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.convbn_2 = nn.BatchNorm2d(num_features=128)\n",
    "\n",
    "        self.conv2d_3 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.convbn_3 = nn.BatchNorm2d(num_features=128)\n",
    "\n",
    "        self.pool_2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.drop_2 = nn.Dropout2d(p=drop_p)\n",
    "\n",
    "        # Third hidden layer\n",
    "        self.conv2d_4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.convbn_4 = nn.BatchNorm2d(num_features=256)\n",
    "        \n",
    "        self.conv2d_5 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.convbn_5 = nn.BatchNorm2d(num_features=256)\n",
    "\n",
    "        self.pool_3 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.drop_3 = nn.Dropout(p=drop_p)\n",
    "\n",
    "        # Dense fully connected layer\n",
    "        self.dense_linear_1 = nn.Linear(256*3*3, 512)\n",
    "        self.drop_4 = nn.Dropout(p=drop_p)\n",
    "\n",
    "        self.dense_linear_2 = nn.Linear(512, 256)\n",
    "        self.drop_5 = nn.Dropout(p=drop_p)\n",
    "\n",
    "        self.dense_linear_3 = nn.Linear(256, 128)\n",
    "        self.out_layer = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Define the feed-forward flow of our neural network\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.conv2d_0(x)\n",
    "        x = self.convbn_0(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        \n",
    "        x = self.conv2d_1(x)\n",
    "        x = self.convbn_1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "\n",
    "        x = self.pool_1(x)\n",
    "        x = self.drop_1(x)\n",
    "\n",
    "        x = self.conv2d_2(x)\n",
    "        x = self.convbn_2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "\n",
    "        x = self.conv2d_3(x)\n",
    "        x = self.convbn_3(x)\n",
    "        x = F.leaky_relu(x)\n",
    "\n",
    "        x = self.pool_2(x)\n",
    "        x = self.drop_2(x)\n",
    "\n",
    "        x = self.conv2d_4(x)\n",
    "        x = self.convbn_4(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        \n",
    "        x = self.conv2d_5(x)\n",
    "        x = self.convbn_5(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        \n",
    "        x = self.pool_3(x)\n",
    "        x = self.drop_3(x)\n",
    "\n",
    "        x = x.view(-1, 256*3*3)\n",
    "        x = self.dense_linear_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.drop_4(x)\n",
    "        \n",
    "        x = self.dense_linear_2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.drop_5(x)\n",
    "        \n",
    "        x = self.dense_linear_3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        out = self.out_layer(x)\n",
    "        return out\n",
    "\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 2 Construct Neural Network Object\n",
    "\n",
    "Now that we have defined our CNN model class using `nn.Module` and added the all layers, let's make it come to life by constructing the object so that we can use it to make prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing our CNN module\n",
    "model = KannadaCNN().to(device)\n",
    "# initialise network\n",
    "net = KannadaCNN().to(device)\n",
    "\n",
    "# optimiser\n",
    "optimiser = optim.Adam(net.parameters(), lr=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# display model summary\n",
    "summary(model, input_size=(1,IMGSIZE,IMGSIZE))  # IMGSIZE = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Training Session\n",
    "\n",
    "Now let's fit our convolutional neural network by using our training set. For every epoch, we will evaluate the performance of our network on validation set and monitor the result. Finally, we perform one more evaluation on our testing set to make sure our setup is not biased towards our validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 1 Evaluation Metric\n",
    "We define a function to help calculate the accuracy metric. Often, while building machine learning models, we focus on the accuracy metrics, trying to get the right class of an image or the right category for a paragraph of text. \n",
    "\n",
    "$$ \\text{Accuracy} = \\frac{\\text{Number of Correct Prediction}}{\\text{Number of all images in Dataset}}\n",
    "$$\n",
    "\n",
    "But these tasks, if only measured on the accuracy of the highest probability prediction limits our understanding of the network and limits the areas it can be applied to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concept of Top-N Accuracy Metric\n",
    "\n",
    "Let's define two terms here.\n",
    "\n",
    "**Top 1 accuracy** — In a classification problem, top1 accuracy method extracts the maximum value out of your final softmax outputs — the value that corresponds to the confidence for the predicted class for your input. Now it means that we choose the predicted class with highest probability as our guess.\n",
    "\n",
    "$$ \\text{Top1} = \\frac{\\text{# of times the correct class has highest probability}}{\\text{Number of all images in Dataset}}\n",
    "$$\n",
    "\n",
    "**Top N accuracy** — Top N accuracy is when a measure of how often your predicted class falls in the top N values of your softmax distribution. In the case of top-5 score, we check if the target label is one of your top 5 predictions (the 5 ones with the highest probabilities).\n",
    "\n",
    "$$ \\text{Top5} = \\frac{\\text{# of times the correct class is in top5 probability}}{\\text{Number of all images in Dataset}}\n",
    "$$\n",
    "\n",
    "If you wanted to know more about evaluation metrics in machine learning, check out a nice course on [courser](https://www.coursera.org/lecture/recommender-metrics/rank-aware-top-n-metrics-Wk98r) from University of Minnesota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 2 Model Fitting\n",
    "\n",
    "Let's define how our training looks like when we fit and run our network to learn on data.\n",
    "\n",
    "1. First we load our data per mini-batch\n",
    "2. Perform lightweight data preprocessing/transformation (under the hood of `torch.DataLoader`)\n",
    "3. Feed forward data to our `CNN`. Let it classify which digit does the image belongs to. Our network make a guess by outputing a probability for each class.\n",
    "4. Calculate the error/loss based on groun truth and our `CNN` prediction.\n",
    "5. Perform gradient descent, optimise weights and parameters update in our `CNN` so that next time, it can learn and make better guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_helper(train_loader, model, optimizer, criterion,\n",
    "                  epoch, device='cpu', log_interval=25):\n",
    "    # set to training mode\n",
    "    model.train()\n",
    "\n",
    "    # training result to record\n",
    "    train_loss = 0.0\n",
    "    train_top1 = 0.0\n",
    "    train_top5 = 0.0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader, start=1):\n",
    "        # convert tensor for current runtime device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # reset optimiser gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # feed forward\n",
    "        out = model(data)\n",
    "        \n",
    "        # calculate loss and optimise network params\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        # optimize weight to account for loss/gradient\n",
    "        optimizer.step()\n",
    "\n",
    "        # calculate training accuracy for top1 and top5\n",
    "        top1, top5 = accuracy(out, target, topk=(1,5))\n",
    "\n",
    "        # update result records\n",
    "        train_top1 += top1.item()\n",
    "        train_top5 += top5.item()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # logging loss output to stdout\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {:03d} [{:05d}/{:05d} ({:2.0f}%)] | '\n",
    "                  'Top1 Acc: {:4.1f} \\t| Top5 Acc: {:4.1f} \\t| Loss: {:.4f}'\n",
    "                  .format(epoch, batch_idx * len(data), len(train_loader.sampler),\n",
    "                      100 * batch_idx / len(train_loader),\n",
    "                      top1, top5, loss.item()))\n",
    "\n",
    "    # display training result\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_top1 /= len(train_loader) # average loss over mini-batches\n",
    "    train_top5 /= len(train_loader) # average loss over mini-batches\n",
    "\n",
    "    print('Training Summary Epoch: {:03d} | '\n",
    "          'Average Top1 Acc: {:.2f}  | Average Top5 Acc: {:.2f} | Loss: {:.4f}'\n",
    "          .format(epoch, train_top1, train_top5, train_loss))\n",
    "    \n",
    "    return train_loss, train_top1, train_top5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Model Evaluation\n",
    "\n",
    "For both validation and testing, the flow will be the same except that we use different images from different set/split.\n",
    "\n",
    "1. First we load our data per mini-batch\n",
    "2. Perform lightweight data preprocessing/transformation (under the hood of `torch.DataLoader`)\n",
    "3. Feed forward data to our `CNN`. Let it classify which digit does the image belongs to. Our network make a guess by outputing a probability for each class.\n",
    "4. Calculate the error/loss, top1 accuracy, and top5 accuracy as our evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_helper(test_loader, model, criterion, \n",
    "                 epoch, device='cpu'):\n",
    "    # set to validation mode\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0.0  # record testing loss\n",
    "    test_top1 = 0.0\n",
    "    test_top5 = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader, start=1):\n",
    "\n",
    "        # convert tensor for current runtime device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # generate image x\n",
    "        out = model(data)\n",
    "\n",
    "        # calculate loss and optimise network params\n",
    "        loss = criterion(out, target)\n",
    "        \n",
    "        # calculate testing accuracy for top1 and top5\n",
    "        top1, top5 = accuracy(out, target, topk=(1,5))\n",
    "\n",
    "        # update test loss\n",
    "        test_top1 += top1.item()\n",
    "        test_top5 += top5.item()\n",
    "        test_loss += loss.item()\n",
    "\n",
    "    # display validation/testing result\n",
    "    test_loss /= len(test_loader.dataset)  # average loss over all images\n",
    "    test_top1 /= len(test_loader)\n",
    "    test_top5 /= len(test_loader)\n",
    "\n",
    "    print('Test Summary Epoch: {:03d} | '\n",
    "          'Average Top1 Acc: {:.2f}  | Average Top5 Acc: {:.2f} | Loss: {:.4f}'\n",
    "          .format(epoch, test_top1, test_top5, test_loss))\n",
    "    \n",
    "    return test_loss, test_top1, test_top5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 4 Running Complete Train Session\n",
    "\n",
    "Finally, combining the training and model evaluation in our flow, we will run the model for several epochs. Below are several training **hyperparameters** you need to be aware of\n",
    "\n",
    "- **`epochs`** : Number of times our `CNN` will see the training data\n",
    "- **`batch size`**: Often called mini-batchsize as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# COMPLETE TRAINING SESSION\n",
    "# ----------------------------\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(1, 2 + 1):\n",
    "    print('\\n' + '-' * 100)\n",
    "    # run session on training set\n",
    "    train_loss, train_acc, _  = train_helper(train_loader, net, optimiser, criterion,\n",
    "                                              epoch=epoch, device=device, log_interval=100)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    # run session on validation set\n",
    "    val_loss, val_acc, _ = test_helper(val_loader, net, criterion, epoch, device=device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "# finally, run session testing set\n",
    "print('\\n' + 'Final Test Set Result:\\n'+ '*' * 80)\n",
    "test_helper(test_loader, net, criterion, epoch, device=device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 5 Visualise Training Session results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n",
    "\n",
    "# Plot Error training vs validation\n",
    "\n",
    "axes[0].set_title('Losses over Epochs')\n",
    "axes[0].plot(train_losses);\n",
    "axes[0].plot(val_losses);\n",
    "\n",
    "axes[0].set_ylabel('Error');\n",
    "axes[0].set_xlabel('Epochs');\n",
    "\n",
    "axes[0].set_ylim(0, 0.005);\n",
    "axes[0].legend(labels=['train error', 'validation error']);\n",
    "\n",
    "# Plot Accuracy training vs validation\n",
    "axes[1].set_title('Accuracy over Epochs')\n",
    "axes[1].plot(train_accuracies);\n",
    "axes[1].plot(val_accuracies);\n",
    "\n",
    "axes[1].set_ylabel('Accuracy (%)');\n",
    "axes[1].set_xlabel('Epochs');\n",
    "\n",
    "axes[1].set_ylim(80, 100);\n",
    "axes[1].legend(labels=['train acc', 'validation acc']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Donahue, J, et al. \"Long-term recurrent convolutional networks for visual recognition and description.\"     Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.\n",
    "\n",
    "[2]Vinyals, Oriol, et al. \"Show and tell: Lessons learned from the 2015 mscoco image captioning challenge.\" IEEE transactions on pattern analysis and machine intelligence 39.4 (2017): 652-663.\n",
    "\n",
    "[3] TensorFlow Show and Tell:A Neural Image Caption Generator [example] (https://github.com/tensorflow/models/tree/master/im2txt)\n",
    "\n",
    "[4] Karapthy, A. [NeuralTalk2](https://github.com/karpathy/neuraltalk2)\n",
    "\n",
    "[5]Lin, Tsung-Yi, et al. \"Microsoft coco: Common objects in context.\" European Conference on Computer Vision. Springer International Publishing, 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
