{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative MNIST model using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Goal](#GoalB)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Host](#Host)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "**Model**: Ever heard about *Generative Adversarial Model* (GAN) ? How about *DeepFakes*? Many generative models nowadays use typical GAN architecture to model data distribution, then reconstruct and generate a completely new data.\n",
    "\n",
    "We have seen many application of GAN such as face generation and face morphing. The underlying model behind GAN architecture that enables its generative capability is called *Variational Autoencoder* (VAE). Today we are going to take a look on how to build a simple Conditional VAE model.\n",
    "\n",
    "**Dataset**: MNIST is a widely used dataset for handwritten digit classification. It consists of 70,000 labeled 28x28 pixel grayscale images of hand-written digits. The dataset is split into 60,000 training images and 10,000 test images. There are 10 classes (one for each of the 10 digits).\n",
    "\n",
    "\n",
    "## Goal\n",
    "This tutorial will show you how to build and deploy on Sagemaker using Pytorch. The dataset we are using is the MNIST dataset. To get the feel, we will first see how a simple neural network can be used to generate a new handwritten digit image without human intervension. Then we will train and deploy our Pytorch model in Sagemaker environment.\n",
    "\n",
    "For more information about the PyTorch in SageMaker, please visit [sagemaker-pytorch-containers](https://github.com/aws/sagemaker-pytorch-containers) and [sagemaker-python-sdk](https://github.com/aws/sagemaker-python-sdk) github repositories.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup \n",
    "_(Duration: 5 min)_\n",
    "\n",
    "_This notebook was created and tested on an ml.m4.xlarge notebook instance._\n",
    "\n",
    "Let's start by creating a SageMaker session and specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the `sagemaker.get_execution_role()` with a the appropriate full IAM role arn string(s).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/workshop-pytorch-mnist'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages\n",
    "\n",
    "We will also setup our project by specifying libraries and modules that we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import json, logging, argparse\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markdown settings for table display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "    table {\n",
    "        display: inline-block\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data: MNIST\n",
    "_(Duration: 10 min)_\n",
    "\n",
    "\n",
    "Downloading may take a few moments, and you should see your progress as the data is loading. You may also choose to change the `batch_size` if you want to load more data at a time.\n",
    "\n",
    "This cell will create DataLoaders for each of our datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify where to download MNIST data\n",
    "data_dir = 'data/'\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 64\n",
    "\n",
    "# convert data to a normalized torch.FloatTensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=(-10,10)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.MNIST(data_dir, train=True, transform=transform, download=True)\n",
    "test_data = datasets.MNIST(data_dir, train=False, transform=transform, download=True)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a Batch of Training Data\n",
    "The first step in a classification task is to take a look at the data, make sure it is loaded in correctly, then make any initial observations about patterns in that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    ax.set_title('digit ' + str(labels[idx].item()))  # .item() gets single value in scalar tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Image in detail\n",
    "\n",
    "Now let's see an image from MNIST dataset in detail. Notice how our image pixels only ranges from $(0, 1)$. This means that no further normalisation is required in the preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.squeeze(images[1])\n",
    "\n",
    "fig = plt.figure(figsize = (12,12)) \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')\n",
    "width, height = img.shape\n",
    "thresh = img.max()/2.5\n",
    "\n",
    "for x in range(width):\n",
    "    for y in range(height):\n",
    "        val = round(img[x][y],2) if img[x][y] !=0 else 0\n",
    "        ax.annotate(str(val), xy=(y,x),\n",
    "                    horizontalalignment='center',\n",
    "                    verticalalignment='center',\n",
    "                    color='white' if img[x][y]<thresh else 'black')\n",
    "\n",
    "ax.set_title('MNIST Digit in detail');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the data to S3\n",
    "Next, we are going to use the `sagemaker.Session.upload_data` function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use later when we start the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_data_path = sagemaker_session.upload_data(path='data', bucket=bucket, key_prefix=prefix)\n",
    "print('Data path/url in AWS S3 -> {}'.format(s3_data_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition: CVAE Neural Network\n",
    "_(Duration: 25 min)_\n",
    "\n",
    "The Conditional Variational Autoencoder (CVAE) architecture will be responsible for seeing input of $784$-dim Tensor of pixel values for each image and $10$-dim Tensor of our label's onehot encoding representation. Our VAE architecture consisted of $7$ hidden layers. each layer is a linear layer with `Weight` and `bias` parameters. \n",
    "\n",
    "Through those parameters, the encoder part of our CVAE model will learn how to decompose an digit image into a latent vector $Z$ of $75$-dim by using the input image from our dataset. We also allow our model to be conditioned for which number we want to generate the image. This is done by passing the labels as part of the input along with our image.\n",
    "\n",
    "Finally our decoder will take the latent vector $Z$ and learn how to re-generate a dataset that is close to the original dataset, but it has little variant in it. The decoder part enables our model to generate a new MNIST digit that is quite similar to the digit in our dataset.\n",
    "\n",
    "Our CVAE model follows the bayesian formulation of VAE as described in the [original paper](https://arxiv.org/pdf/1812.04405) conditioned on additive information (in our case, the classification label of our data). Checkout the original paper if you're interested in the mathematics behind CVAE and how it empowered many application in real world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In VAE there are two modules:\n",
    "\n",
    "- Encoder: $Q_\\phi(Z_i | X_i)$\n",
    "- Decoder: $P_\\theta(X_i|Z_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Module\n",
    "\n",
    "We define our Encoder $Q_\\phi(Z_i | X_i)$ layer as follows:\n",
    "\n",
    "\n",
    "\n",
    "Layer name           |Configuration| \n",
    ":---                 |:---         |\n",
    "input_layer          | FC-794      |\n",
    "hidden_layer_0       | FC-512      | \n",
    "hidden_layer_1       | FC-128      |  \n",
    "hidden_layer_2       | FC-128      | \n",
    "bottle_neck_layer    | FC-75       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our MNIST image is $28 \\times 28$. When we flatten the image, it becomes a  long vector of $784$-dim. We concatenate this vector with onehot encoding of our label which is a $10$-dim vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In between each layer, we will use *Rectified Linear unit* (ReLU) as our activation function. ReLU is defined mathematically as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\" Encoder module of simple Conditional VAE network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, latent_dim, num_classes=10):\n",
    "        \"\"\" Initialise Encoder component of CVAE\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Initialise first hidden layer\n",
    "        self.hidden_layer_0 = nn.Linear(input_dim, 512)    # TODO: specify hidden layer 0\n",
    "        self.hidden_layer_1 = nn.Linear(512, 128)          # TODO: specify hidden layer 1\n",
    "        self.hidden_layer_2 = nn.Linear(128, 128)          # TODO: specify hidden layer 2\n",
    "\n",
    "        # Bottle neck layers\n",
    "        self.mu_layer = nn.Linear(128, latent_dim)         # TODO: specfiy bottle-neck layer\n",
    "        self.logvar_layer = nn.Linear(128, latent_dim)     # TODO: specify bottle-neck layer\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x shape: [batch_size, input_dim + num_classes]\n",
    "        x = x.view(-1, self.input_dim + self.num_classes)\n",
    "\n",
    "        x = F.relu(self.hidden_layer_0(x))\n",
    "        x = F.relu(self.hidden_layer_1(x))\n",
    "        x = F.relu(self.hidden_layer_2(x))\n",
    "\n",
    "        # Latent space parametric variables\n",
    "        z_mu = self.mu_layer(x)\n",
    "        z_logvar = self.logvar_layer(x)\n",
    "\n",
    "        return z_mu, z_logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Module\n",
    "\n",
    "We define our Decoder Decoder $P_\\theta(X_i|Z_i)$ layer as follows:\n",
    "\n",
    "\n",
    "Layer name           |Configuration|\n",
    ":---                 |:---         |\n",
    "hidden_layer_3       | FC-128      |\n",
    "hidden_layer_4       | FC-128      |\n",
    "hidden_layer_5       | FC-512      |\n",
    "reconstruction_layer | FC-784      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\" Decoder module for Conditional VAE network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, latent_dim, num_classes=10):\n",
    "        \"\"\" Initialise Decoder component of CVAE\n",
    "        Arguments:\n",
    "        ------------------------\n",
    "            .. input_dim (int)   - input channel of data X, the length of flattened image vector\n",
    "            .. latent_dim (int)  - length of latent dim vector in bottle neck layer\n",
    "            .. num_classes (int) - number of classes/target groups in dataset\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Initialise first hidden layer\n",
    "        self.hidden_layer_3 = nn.Linear(input_dim, 128)    # TODO: specify hidden layer 3\n",
    "        self.hidden_layer_4 = nn.Linear(128, 128)          # TODO: specify hidden layer 4\n",
    "        self.hidden_layer_5 = nn.Linear(128, 512)          # TODO: specify hidden layer 5\n",
    "\n",
    "        # Recontruction layer\n",
    "        self.recontruction_layer = nn.Linear(512, input_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Latent variable z shape: [batch_size, latent_dim + num_classes]\n",
    "        x = z.view(-1, self.latent_dim + self.num_classes)\n",
    "\n",
    "        # Forward flow to hidden layers\n",
    "        x = F.relu(self.hidden_layer_3(x))\n",
    "        x = F.relu(self.hidden_layer_4(x))\n",
    "        x = F.relu(self.hidden_layer_5(x))\n",
    "\n",
    "        # Recontructed input image\n",
    "        generated_x = F.sigmoid(self.recontruction_layer(x))\n",
    "\n",
    "        return generated_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional VAE Module\n",
    "\n",
    "Now that we have defined the Encoder and Decoder, letâ€™s combine them into our Conditional VAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    \"\"\" Conditional VAE (Variational Autoencoder)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, latent_dim, num_classes):\n",
    "        \"\"\" Initialise Decoder component of CVAE\n",
    "        Arguments:\n",
    "        ------------------------\n",
    "            .. input_dim (int) - input channel of data X, the length of flattened image vector\n",
    "            .. latent_dim (int) - length of latent dim vector in bottle neck layer\n",
    "            .. num_classes (int) - number of classes/target groups in dataset\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = CEncoder(input_dim, latent_dim, num_classes)\n",
    "        self.decoder = CDecoder(torch_mnist/input_dim, latent_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # Concatenate image x and label y\n",
    "        x = torch.cat((x, y), dim=1)\n",
    "\n",
    "        # Learn latent Z distribution parameter\n",
    "        z_mu, z_logvar = self.encoder(x)\n",
    "\n",
    "        # Resampling using reparameterisation trick\n",
    "        std = torch.exp(z_logvar / 2)\n",
    "        eps = torch.randn_like(std)\n",
    "        sampled_z = eps * std + z_mu\n",
    "        \n",
    "        z = torch.cat((sampled_z, y), dim=1)\n",
    "\n",
    "        # Recontruct image x\n",
    "        generated_x = self.decoder(z)\n",
    "\n",
    "        return generated_x, z_mu, z_logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Session in Cloud\n",
    "*(Duration: 20 min)*\n",
    "\n",
    "### Training script\n",
    "The `mnist.py` script provides all the code we need for training and hosting a SageMaker model (`model_fn` function to load a model).\n",
    "The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, such as:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string representing the path to the directory to write model artifacts to.\n",
    "  These artifacts are uploaded to S3 for model hosting.\n",
    "* `SM_NUM_GPUS`: The number of gpus available in the current container.\n",
    "* `SM_CURRENT_HOST`: The name of the current container on the container network.\n",
    "* `SM_HOSTS`: JSON encoded list containing all the hosts .\n",
    "\n",
    "Supposing one input channel, 'training', was used in the call to the PyTorch estimator's `fit()` method, the following will be set, following the format `SM_CHANNEL_[channel_name]`:\n",
    "\n",
    "* `SM_CHANNEL_TRAINING`: A string representing the path to the directory containing data in the 'training' channel.\n",
    "\n",
    "For more information about training environment variables, please visit [SageMaker Containers](https://github.com/aws/sagemaker-containers).\n",
    "\n",
    "A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to `model_dir` so that it can be hosted later. Hyperparameters are passed to your script as arguments and can be retrieved with an `argparse.ArgumentParser` instance.\n",
    "\n",
    "Because the SageMaker imports the training script, you should put your training code in a main guard (``if __name__=='__main__':``) if you are using the same script to host your model as we do in this example, so that SageMaker does not inadvertently run your training code at the wrong point in execution.\n",
    "\n",
    "For the script run by this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize entrypoint.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Distributed Computing in Sagemaker\n",
    "\n",
    "The `PyTorch` class allows us to run our training function as a training job on SageMaker infrastructure. We need to configure it with our training script, an IAM role, the number of training instances, the training instance type, and hyperparameters. In this case we are going to run our training job on 2 ```ml.c4.xlarge``` instances. But this example can be ran on one or multiple, cpu or gpu instances ([full list of available instances](https://aws.amazon.com/sagemaker/pricing/instance-types/)). The hyperparameters parameter is a dict of values that will be passed to your training script -- you can see how to access these values in the `entrypoint.py` script above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point='entrypoint.py',    # script for training entrypoint\n",
    "                    dependencies=['torch_mnist'],   # source code for model and other functionals\n",
    "                    role=role,\n",
    "                    framework_version='1.2.0',\n",
    "                    train_instance_count=2,\n",
    "                    train_instance_type='ml.c4.xlarge',\n",
    "                    hyperparameters={\n",
    "                        'epochs': 3,\n",
    "                        'backend': 'gloo'\n",
    "                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've constructed our `PyTorch` object, we can fit it using the data we uploaded to S3. SageMaker makes sure our data is available in the local filesystem, so our training script can simply read the data from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit({'training': s3_data_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host\n",
    "After training, we use the `PyTorch` estimator object to build and deploy a `PyTorchPredictor`. This creates a Sagemaker Endpoint -- a hosted prediction service that we can use to perform inference.\n",
    "\n",
    "As mentioned above we have implementation of `model_fn` in the `mnist.py` script that is required. We are going to use default implementations of `input_fn`, `predict_fn`, `output_fn` and `transform_fm` defined in [sagemaker-pytorch-containers](https://github.com/aws/sagemaker-pytorch-containers).\n",
    "\n",
    "The arguments to the deploy function allow us to set the number and type of instances that will be used for the Endpoint. These do not need to be the same as the values we used for the training job. For example, you can train a model on a set of GPU-based instances, and then deploy the Endpoint to a fleet of CPU-based instances, but you need to make sure that you return or save your model as a cpu model similar to what we did in `mnist.py`. Here we will deploy the model to a single ```ml.m4.xlarge``` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "We can now use this predictor to classify hand-written digits. Drawing into the image box loads the pixel data into a `data` variable in this notebook, which we can then pass to the `predictor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "image = np.array(test_images[0], dtype=np.uint8) / 255\n",
    "image = image.reshape(1, 1, 28, 28)\n",
    "response = predictor.predict(image)\n",
    "prediction = response.argmax(axis=1)[0]\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "After you have finished with this example, remember to delete the prediction endpoint to release the instance(s) associated with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
